{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine_tune_0.csv ddl0.sql\n",
      "fine_tune_1.csv ddl1.sql\n",
      "fine_tune_2.csv ddl2.sql\n",
      "fine_tune_3.csv ddl3.sql\n",
      "fine_tune_4.csv ddl4.sql\n",
      "fine_tune_5.csv ddl5.sql\n",
      "fine_tune_6.csv ddl6.sql\n",
      "fine_tune_7.csv ddl7.sql\n",
      "fine_tune_8.csv ddl8.sql\n",
      "fine_tune_9.csv ddl9.sql\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csv_dir = '/home/ksaff/Desktop/ttyd/fine_tuning/llama_finetuning/10_datasets'\n",
    "ddl_dir = '/home/ksaff/Desktop/ttyd/fine_tuning/fine_tune_dbs/ddls'\n",
    "\n",
    "csv_names = sorted(os.listdir(csv_dir))[:10]\n",
    "ddl_names = sorted(os.listdir(ddl_dir))\n",
    "ddl_names.pop(2) #Del DDL10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def rewrite_DDL(DDL):\n",
    "    lines = DDL.split('\\n')\n",
    "    modified_text = \"DDL: \"\n",
    "    for line in lines:\n",
    "        if len(line) < 2:\n",
    "            pass\n",
    "        elif '/*' in line:\n",
    "            pass\n",
    "        elif 'DROP TABLE' in line:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            line = re.sub(r'(?<!NOT )\\bNULL\\b', '', line)\n",
    "            modified_text += f'{line}\\n'\n",
    "            \n",
    "    return modified_text\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for csv_file, ddl_file in zip(csv_names, ddl_names):\n",
    "    csv_path = os.path.join(csv_dir, csv_file)\n",
    "    ddl_path = os.path.join(ddl_dir, ddl_file)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    with open(ddl_path) as file:\n",
    "        ddl = file.read()\n",
    "    ddl = rewrite_DDL(ddl)\n",
    "    df = df.T.reset_index().T\n",
    "    new_columns = ['instruction', 'output']\n",
    "    df.columns = new_columns\n",
    "    df.index = range(0, len(df))\n",
    "    df['input'] = ddl\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    prompt = 'Make SQLite query based on DDL and instruction.'\n",
    "    instruction = row['instruction']\n",
    "    input_query = row['input']\n",
    "    response = row['output']\n",
    "\n",
    "    text = (\n",
    "        prompt\n",
    "        + '### Instruction:\\n'\n",
    "        + instruction\n",
    "        + '### Input:\\n'\n",
    "        + input_query\n",
    "        + '### Response:\\n'\n",
    "        + response\n",
    "    )\n",
    "\n",
    "    text_col.append(text)\n",
    "\n",
    "df['text'] = text_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train.csv', index=False)\n",
    "# df[20:].to_csv('train.csv', index=False)\n",
    "# df[:20].to_csv('test.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
