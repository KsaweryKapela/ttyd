{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qJ90LnMv54Y-"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/CodeLlama-13B-Instruct-GGUF\"\n",
        "model_basename = \"codellama-13b-instruct.Q6_K.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=on\"\n",
        "os.environ[\"FORCE_CMAKE\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ak3ZtGjM6Wdp"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 4871.43it/s]\n",
            "Fetching 0 files: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Model file 'codellama-13b-instruct.q4_K_M.gguf' not found in '/home/ksaff/.cache/huggingface/hub/models--TheBloke--CodeLlama-13B-Instruct-GGUF/snapshots/82f1dd9567b9b20b7e8f8aa9ecf3d2f121e5d415'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mctransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m llm \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mTheBloke/CodeLlama-13B-Instruct-GGUF\u001b[39;49m\u001b[39m\"\u001b[39;49m, model_file\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcodellama-13b-instruct.q4_K_M.gguf\u001b[39;49m\u001b[39m\"\u001b[39;49m, model_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mllama\u001b[39;49m\u001b[39m\"\u001b[39;49m, gpu_layers\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(llm(\u001b[39m\"\u001b[39m\u001b[39mAI is going to\u001b[39m\u001b[39m\"\u001b[39m))\n",
            "File \u001b[0;32m~/miniconda3/envs/llm_jupyter/lib/python3.10/site-packages/ctransformers/hub.py:168\u001b[0m, in \u001b[0;36mAutoModelForCausalLM.from_pretrained\u001b[0;34m(cls, model_path_or_repo_id, model_type, model_file, config, lib, local_files_only, revision, hf, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     model_path \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_find_model_path_from_dir(\n\u001b[1;32m    165\u001b[0m         model_path_or_repo_id, model_file\n\u001b[1;32m    166\u001b[0m     )\n\u001b[1;32m    167\u001b[0m \u001b[39melif\u001b[39;00m path_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrepo\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 168\u001b[0m     model_path \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_find_model_path_from_repo(\n\u001b[1;32m    169\u001b[0m         model_path_or_repo_id,\n\u001b[1;32m    170\u001b[0m         model_file,\n\u001b[1;32m    171\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    172\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    173\u001b[0m     )\n\u001b[1;32m    175\u001b[0m llm \u001b[39m=\u001b[39m LLM(\n\u001b[1;32m    176\u001b[0m     model_path\u001b[39m=\u001b[39mmodel_path,\n\u001b[1;32m    177\u001b[0m     model_type\u001b[39m=\u001b[39mmodel_type,\n\u001b[1;32m    178\u001b[0m     config\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mconfig,\n\u001b[1;32m    179\u001b[0m     lib\u001b[39m=\u001b[39mlib,\n\u001b[1;32m    180\u001b[0m )\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m hf:\n",
            "File \u001b[0;32m~/miniconda3/envs/llm_jupyter/lib/python3.10/site-packages/ctransformers/hub.py:209\u001b[0m, in \u001b[0;36mAutoModelForCausalLM._find_model_path_from_repo\u001b[0;34m(cls, repo_id, filename, local_files_only, revision)\u001b[0m\n\u001b[1;32m    202\u001b[0m allow_patterns \u001b[39m=\u001b[39m filename \u001b[39mor\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39m*.bin\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m*.gguf\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    203\u001b[0m path \u001b[39m=\u001b[39m snapshot_download(\n\u001b[1;32m    204\u001b[0m     repo_id\u001b[39m=\u001b[39mrepo_id,\n\u001b[1;32m    205\u001b[0m     allow_patterns\u001b[39m=\u001b[39mallow_patterns,\n\u001b[1;32m    206\u001b[0m     local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    207\u001b[0m     revision\u001b[39m=\u001b[39mrevision,\n\u001b[1;32m    208\u001b[0m )\n\u001b[0;32m--> 209\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_find_model_path_from_dir(path, filename\u001b[39m=\u001b[39;49mfilename)\n",
            "File \u001b[0;32m~/miniconda3/envs/llm_jupyter/lib/python3.10/site-packages/ctransformers/hub.py:242\u001b[0m, in \u001b[0;36mAutoModelForCausalLM._find_model_path_from_dir\u001b[0;34m(cls, path, filename)\u001b[0m\n\u001b[1;32m    240\u001b[0m     file \u001b[39m=\u001b[39m (path \u001b[39m/\u001b[39m filename)\u001b[39m.\u001b[39mresolve()\n\u001b[1;32m    241\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m file\u001b[39m.\u001b[39mis_file():\n\u001b[0;32m--> 242\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not found in \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(file)\n\u001b[1;32m    245\u001b[0m files \u001b[39m=\u001b[39m [\n\u001b[1;32m    246\u001b[0m     (f\u001b[39m.\u001b[39mstat()\u001b[39m.\u001b[39mst_size, f)\n\u001b[1;32m    247\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m path\u001b[39m.\u001b[39miterdir()\n\u001b[1;32m    248\u001b[0m     \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mis_file() \u001b[39mand\u001b[39;00m (f\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.bin\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m f\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.gguf\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    249\u001b[0m ]\n",
            "\u001b[0;31mValueError\u001b[0m: Model file 'codellama-13b-instruct.q4_K_M.gguf' not found in '/home/ksaff/.cache/huggingface/hub/models--TheBloke--CodeLlama-13B-Instruct-GGUF/snapshots/82f1dd9567b9b20b7e8f8aa9ecf3d2f121e5d415'"
          ]
        }
      ],
      "source": [
        "from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
        "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/CodeLlama-13B-Instruct-GGUF\", model_file=\"codellama-13b-instruct.q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\n",
        "\n",
        "print(llm(\"AI is going to\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irftToUj6aWt",
        "outputId": "17581285-8cda-4e10-db1f-af6b51f1365d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama.cpp: loading model from /home/ksaff/.cache/huggingface/hub/models--TheBloke--CodeLlama-13B-Instruct-GGUF/snapshots/82f1dd9567b9b20b7e8f8aa9ecf3d2f121e5d415/codellama-13b-instruct.Q6_K.gguf\n",
            "error loading model: unknown (magic, version) combination: 46554747, 00000002; is this really a GGML file?\n",
            "llama_load_model_from_file: failed to load model\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# GPU\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lcpp_llm \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lcpp_llm \u001b[39m=\u001b[39m Llama(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model_path\u001b[39m=\u001b[39;49mmodel_path,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     n_threads\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m# CPU cores\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     n_batch\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m, \u001b[39m# Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     n_gpu_layers\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m \u001b[39m# Change this value based on your model and your GPU VRAM pool.\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/llm_jupyter/lib/python3.10/site-packages/llama_cpp/llama.py:328\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split, rope_freq_base, rope_freq_scale, n_gqa, rms_norm_eps, mul_mat_q, verbose)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[39mwith\u001b[39;00m suppress_stdout_stderr():\n\u001b[1;32m    325\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39mllama_load_model_from_file(\n\u001b[1;32m    326\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_path\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\n\u001b[1;32m    327\u001b[0m         )\n\u001b[0;32m--> 328\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m    331\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39mllama_new_context_with_model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# GPU\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG4Pylz662At",
        "outputId": "a8915ee7-2171-404f-fe8f-7184547d8f77"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'params'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# See the number of layers in GPU\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ksaff/Desktop/ttyd/llm/code_llm.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lcpp_llm\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39mn_gpu_layers\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'params'"
          ]
        }
      ],
      "source": [
        "# See the number of layers in GPU\n",
        "lcpp_llm.params.n_gpu_layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE-M307R6_pT"
      },
      "source": [
        "#**Step 5: Create a Prompt Template**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfzwELMC7Dyg"
      },
      "outputs": [],
      "source": [
        "prompt = \"Give me user with id 1\"\n",
        "prompt_template=f'''SYSTEM: You only answer with SQL queries.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT8pg6zt7QzA"
      },
      "source": [
        "#**Step 6: Generating the Response**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aF0qWUJ7OPK"
      },
      "outputs": [],
      "source": [
        "response=lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,\n",
        "                  repeat_penalty=1.2, top_k=150,\n",
        "                  echo=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlJ1JgR68DDO",
        "outputId": "5a781d33-fb83-43af-be2d-3ac8856d7280"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qona58gX8oAn",
        "outputId": "64d45fb1-9790-4af7-88bf-d3ed0757c44b"
      },
      "outputs": [],
      "source": [
        "print(response[\"choices\"][0][\"text\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10524e48f4b547718f9892a97b74aedf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92212bf063814297b2cab5b5fd7debb4",
              "IPY_MODEL_770db95a57f743f0a3e792f7958892ad",
              "IPY_MODEL_a7700b22453c413893e67246ce7a46d4"
            ],
            "layout": "IPY_MODEL_588a5efd92c14a889a155f09e9e5172c"
          }
        },
        "588a5efd92c14a889a155f09e9e5172c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c5b443689d74809aacec6cdf5d06b51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c72e588e98f4b4eb16d1b00861747e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a380d81c1e4631bbe90c81f485e610": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "770db95a57f743f0a3e792f7958892ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c72e588e98f4b4eb16d1b00861747e6",
            "max": 9763701888,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e81ef593c3e40289d830e07f3812e7e",
            "value": 9763701888
          }
        },
        "8719b31162724024b14beedcb6c36dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91db638aecc54d8691406925dedd5ca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92212bf063814297b2cab5b5fd7debb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8719b31162724024b14beedcb6c36dd0",
            "placeholder": "​",
            "style": "IPY_MODEL_91db638aecc54d8691406925dedd5ca3",
            "value": "Downloading (…)chat.ggmlv3.q5_1.bin: 100%"
          }
        },
        "9e81ef593c3e40289d830e07f3812e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7700b22453c413893e67246ce7a46d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c5b443689d74809aacec6cdf5d06b51",
            "placeholder": "​",
            "style": "IPY_MODEL_71a380d81c1e4631bbe90c81f485e610",
            "value": " 9.76G/9.76G [01:09&lt;00:00, 238MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
